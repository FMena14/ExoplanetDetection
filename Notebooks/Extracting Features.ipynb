{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koi_light_curves_full.npy  koi_light_curves_model_full.npy  README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../Processed_Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICA\t\t    kepler_downloaded.txt  koi_sets.csv  PCA\r\n",
      "kepler_dataset.csv  koi_metadata.csv\t   OwnFats\t README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../Datos/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, warnings,time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_sets = pd.read_csv(\"../Datos/koi_sets_unb.csv\")\n",
    "mask_train = (df_sets[\"Set\"] == \"Train\").values\n",
    "mask_test = (df_sets[\"Set\"] == \"Test\").values\n",
    "\n",
    "#lc_total = np.load(\"../../Processed_Data/koi_light_curves_full.npy\") #Raw light curve\n",
    "lc_total = np.load(\"../../Processed_Data/koi_light_curves_model_full.npy\")\n",
    "lc_train = lc_total[mask_train] \n",
    "lc_test = lc_total[mask_test]\n",
    "\n",
    "file_name_metadata = \"koi_metadata_p_error.csv\"\n",
    "df_label = pd.read_csv(\"../Datos/\"+file_name_metadata)\n",
    "df_label_train = df_label[mask_train] \n",
    "df_label_test = df_label[mask_test]\n",
    "print(\"Read Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourier transformation done\n",
      "CPU times: user 6min 40s, sys: 4.39 s, total: 6min 44s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fourier\n",
    "X_train = lc_train\n",
    "X_test = lc_test\n",
    "Xtrain_fourier = np.abs(np.fft.fft(X_train))\n",
    "Xtest_fourier = np.abs(np.fft.fft(X_test))\n",
    "print(\"Fourier transformation done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "---\n",
    "#### Generate 3 files in 3 dimension: 5, 10, 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CenterScalar done!\n",
      "Data scaled!\n",
      "CPU times: user 5.49 s, sys: 3.41 s, total: 8.9 s\n",
      "Wall time: 7.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA,PCA,FastICA\n",
    "\n",
    "std = StandardScaler(with_mean=True,with_std=False) #without variance remove --needed\n",
    "std.fit(Xtrain_fourier)\n",
    "print(\"CenterScalar done!\")\n",
    "\n",
    "Xtrain_std = std.transform(Xtrain_fourier)\n",
    "Xtest_std = std.transform(Xtest_fourier)\n",
    "print(\"Data scaled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA training with 5 features\n",
      "Generate CSV of koi_light_curves_PCA_train 5\n",
      "Generate CSV of koi_light_curves_PCA_test 5\n",
      "PCA training with 10 features\n",
      "Generate CSV of koi_light_curves_PCA_train 10\n",
      "Generate CSV of koi_light_curves_PCA_test 10\n",
      "PCA training with 25 features\n",
      "Generate CSV of koi_light_curves_PCA_train 25\n",
      "Generate CSV of koi_light_curves_PCA_test 25\n",
      "Process of dimensionality reduction completed\n",
      "CPU times: user 4min 8s, sys: 9.33 s, total: 4min 18s\n",
      "Wall time: 25.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dims= [5,10,25]\n",
    "\n",
    "for d in dims:\n",
    "    model = PCA(d)\n",
    "    \n",
    "    ##################### TRAIN #####################\n",
    "    model.fit(Xtrain_std) \n",
    "    print('PCA training with', d, 'features')\n",
    "\n",
    "    #################### ARRIBA ENTRENAMIENTO ###########\n",
    "    df = pd.DataFrame(model.transform(Xtrain_std),columns=[\"Component \"+str(t+1) for t in np.arange(d)])\n",
    "    df.to_csv(\"../Datos/PCA_unb/koi_light_curves_FPCA_train\"+str(d)+\"_model.csv\",index=False)\n",
    "    print('Generate CSV of koi_light_curves_PCA_train', d)\n",
    "    \n",
    "    ##################### TEST ########################\n",
    "    df = pd.DataFrame(model.transform(Xtest_std),columns=[\"Component \"+str(t+1) for t in np.arange(d)])\n",
    "    df.to_csv(\"../Datos/PCA_unb/koi_light_curves_FPCA_test\"+str(d)+\"_model.csv\",index=False)\n",
    "    print('Generate CSV of koi_light_curves_PCA_test', d)\n",
    "    \n",
    "print('Process of dimensionality reduction completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA\n",
    "---\n",
    "#### Generate 3 files in 3 dimension: 5, 10 and 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
       "    n_components=5, random_state=None, tol=0.0001, w_init=None,\n",
       "    whiten=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastICA(5,whiten=True)\n",
    "#model.fit(Xtrain_std)  #with std -- no funciona\n",
    "#model.fit(Xtrain_std2) #without std -- funciona \n",
    "#model.fit(Xtrain_fourier) #fft -- funciona\n",
    "model.fit(X_train)  #light curve -- funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICA trainig with 5 features\n",
      "Generate CSV Train 5\n",
      "Generate CSV Test 5\n",
      "ICA trainig with 10 features\n",
      "Generate CSV Train 10\n",
      "Generate CSV Test 10\n",
      "ICA trainig with 15 features\n",
      "Generate CSV Train 15\n",
      "Generate CSV Test 15\n",
      "Process of dimensionality reduction completed\n",
      "CPU times: user 1h 7min 30s, sys: 4min 54s, total: 1h 12min 25s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "dims= [5,10,15]\n",
    "\n",
    "for d in dims:\n",
    "    model = FastICA(d,whiten=True)\n",
    "    \n",
    "    ##################### TRAIN #####################\n",
    "    model.fit(Xtrain_fourier)\n",
    "    print('ICA trainig with', d, 'features')\n",
    "\n",
    "    #################### ARRIBA ENTRENAMIENTO ###########\n",
    "    df = pd.DataFrame(model.transform(Xtrain_fourier),columns=[\"Component \"+str(t+1) for t in np.arange(d)])\n",
    "    df.to_csv(\"../Datos/ICA_unb/koi_light_curves_FICA_train\"+str(d)+\"_model.csv\",index=False)\n",
    "    print('Generate CSV Train', d)\n",
    "\n",
    "    ##################### TEST ########################\n",
    "    df = pd.DataFrame(model.transform(Xtest_fourier),columns=[\"Component \"+str(t+1) for t in np.arange(d)])\n",
    "    df.to_csv(\"../Datos/ICA_unb/koi_light_curves_FICA_test\"+str(d)+\"_model.csv\",index=False)\n",
    "    print('Generate CSV Test', d)\n",
    "    \n",
    "print('Process of dimensionality reduction completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OwnFATS (Manual)\n",
    "---\n",
    "#### Generate Own FATS features of light curve (manualy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import FATS\n",
    "\n",
    "time_ex = np.arange(3000)\n",
    "magnitude_ex = np.random.rand(3000)\n",
    "\n",
    "lc_example = np.array([magnitude_ex, time_ex])\n",
    "\n",
    "time_start = time.time()\n",
    "#a = FATS.FeatureSpace(Data=['magnitude','time'],excludeList= ['SlottedA_length','PeriodLS','StetsonK_AC',\"Period_fit\",\"Psi_CS\",\"Psi_eta\"])\n",
    "#SACAR DE EXCLUDE PERIODLS\n",
    "a = a.calculateFeature(lc_example)\n",
    "print(\"Termino en %f segundos\"%(time.time()-time_start))\n",
    "a.result(method='dict')\n",
    "\n",
    "#it take too long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.signal import resample\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def amplitude(magnitudes):\n",
    "    return 0.5 * (np.max(magnitudes) - np.min(magnitudes))\n",
    "\n",
    "def median_absolute_deviation(magnitudes):\n",
    "    median = np.median(magnitudes)\n",
    "    deviations = magnitudes - median\n",
    "    absolute_deviations = np.absolute(deviations)\n",
    "    return np.median(absolute_deviations)\n",
    "\n",
    "def residual_bright_faint_ratio(magnitudes):    # median as a fit\n",
    "    mean = np.mean(magnitudes)\n",
    "    brighter = magnitudes[magnitudes > mean]\n",
    "    fainter = magnitudes[magnitudes < mean]\n",
    "\n",
    "    resid_brighter = np.mean(np.square(brighter - mean))\n",
    "    resid_fainter = np.mean(np.square(fainter - mean))\n",
    "\n",
    "    ratio = resid_fainter / (resid_brighter+1e-14)\n",
    "    return ratio\n",
    "\n",
    "def own_fats(sequence):\n",
    "    time_ex =  np.arange(len(sequence))\n",
    "\n",
    "    minim=np.min(sequence)\n",
    "    maxim = np.max(sequence)\n",
    "    mean = np.mean(sequence)\n",
    "    std = np.std(sequence)\n",
    "    iqr = stats.iqr(sequence) #q31\n",
    "    skew = stats.skew(sequence)\n",
    "    kurt = stats.kurtosis(sequence)\n",
    "    q1 = np.percentile(sequence, 25)\n",
    "    q2 = np.percentile(sequence, 50)\n",
    "    model = LinearRegression(normalize=True,n_jobs=-1)\n",
    "    model.fit(time_ex.reshape(-1,1),sequence)\n",
    "    slope = model.coef_[0]\n",
    "    #new features\n",
    "    ampl = amplitude(sequence)\n",
    "    mad = median_absolute_deviation(sequence)\n",
    "    br_fa = residual_bright_faint_ratio(sequence)\n",
    "    median = np.median(sequence)\n",
    "    return np.array([minim,maxim,mean,std,iqr,skew,kurt,q1,q2,slope,ampl,mad,br_fa,median])\n",
    "\n",
    "def metadata_columns(match,array):\n",
    "    df = pd.read_csv(\"../Datos/\"+file_name_metadata)\n",
    "    metadata = df[(df[\"KOI Name\"] == match)].values[0][3:]\n",
    "    return np.hstack((array,metadata))\n",
    "\n",
    "columns_FATS = [\"Minimum\",\"Maximum\",\"Mean\",\"Std\",\"IQR\",\"Skew\",\"Kurtosis\",\"Q1\",\"Q2\",\"Slope\",\"Amplitude\",\"MAD\",\n",
    "               \"Residual Bright Faint Ratio\",\"Median\"]\n",
    "aux = pd.read_csv(\"../Datos/\"+file_name_metadata)\n",
    "columns_metadata = list(aux.columns[3:])\n",
    "\n",
    "def extract_FATS(X,df_label):\n",
    "    X_fats = []\n",
    "    for sequence,match in zip(X,df_label[\"KOI Name\"]):\n",
    "        aux = own_fats(sequence)\n",
    "        final = metadata_columns(match,aux)\n",
    "        X_fats.append(final)\n",
    "    return np.asarray(X_fats)\n",
    "\n",
    "def save(name_set,features,df_label):\n",
    "    df2save = pd.DataFrame(features,columns=columns_FATS+columns_metadata)\n",
    "    df2save[\"KOI Name\"] = df_label[\"KOI Name\"]\n",
    "    df2save.to_csv(\"../Datos/OwnFats_unb/koi_light_curves_FATS2_metadata_\"+name_set+\"_p_error.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 21min 21s, sys: 6min 26s, total: 2h 27min 47s\n",
      "Wall time: 6min 9s\n",
      "Training already extracted\n",
      "CPU times: user 46min 49s, sys: 2min 7s, total: 48min 57s\n",
      "Wall time: 2min 2s\n",
      "Validation already extracted\n"
     ]
    }
   ],
   "source": [
    "%time features = extract_FATS(lc_train,df_label_train) #train\n",
    "save(\"train_model\",features,df_label_train)\n",
    "print(\"Training already extracted\")\n",
    "\n",
    "%time features = extract_FATS(lc_test,df_label_test) #test\n",
    "save(\"test_model\",features,df_label_test)\n",
    "print(\"Validation already extracted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
