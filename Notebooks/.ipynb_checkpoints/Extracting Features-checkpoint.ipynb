{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koi_light_curves_full.npy  koi_light_curves_model_full.npy  README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../Processed_Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICA\t\t    kepler_downloaded.txt  koi_sets.csv  PCA\r\n",
      "kepler_dataset.csv  koi_metadata.csv\t   OwnFats\t README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../Datos/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, warnings,time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_sets = pd.read_csv(\"../Datos/koi_sets_unb.csv\")\n",
    "mask_train = (df_sets[\"Set\"] == \"Train\").values\n",
    "mask_test = (df_sets[\"Set\"] == \"Test\").values\n",
    "\n",
    "#lc_total = np.load(\"../../Processed_Data/koi_light_curves_full.npy\") #Raw light curve\n",
    "lc_total = np.load(\"../../Processed_Data/koi_light_curves_model_full.npy\")\n",
    "lc_train = lc_total[mask_train] \n",
    "lc_test = lc_total[mask_test]\n",
    "\n",
    "file_name_metadata = \"koi_metadata_p_error.csv\"\n",
    "df_label = pd.read_csv(\"../Datos/\"+file_name_metadata)\n",
    "df_label_train = df_label[mask_train] \n",
    "df_label_test = df_label[mask_test]\n",
    "print(\"Read Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourier transformation done\n",
      "CPU times: user 6min 43s, sys: 4.81 s, total: 6min 47s\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fourier\n",
    "X_train = lc_train\n",
    "X_test = lc_test\n",
    "Xtrain_fourier = np.abs(np.fft.fft(X_train))\n",
    "Xtest_fourier = np.abs(np.fft.fft(X_test))\n",
    "print(\"Fourier transformation done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "---\n",
    "#### Generate 3 files in 3 dimension: 5, 10, 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CenterScalar done!\n",
      "Data scaled!\n",
      "CPU times: user 4.25 s, sys: 3.57 s, total: 7.81 s\n",
      "Wall time: 6.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA,PCA,FastICA\n",
    "\n",
    "std = StandardScaler(with_mean=True,with_std=False) #without variance remove --needed\n",
    "std.fit(Xtrain_fourier)\n",
    "print(\"CenterScalar done!\")\n",
    "\n",
    "Xtrain_std = std.transform(Xtrain_fourier)\n",
    "Xtest_std = std.transform(Xtest_fourier)\n",
    "print(\"Data scaled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA training with 50 features\n",
      "Generate CSV of koi_light_curves_PCA_train 50\n",
      "Generate CSV of koi_light_curves_PCA_test 50\n",
      "PCA training with 100 features\n",
      "Generate CSV of koi_light_curves_PCA_train 100\n",
      "Generate CSV of koi_light_curves_PCA_test 100\n",
      "PCA training with 250 features\n",
      "Generate CSV of koi_light_curves_PCA_train 250\n",
      "Generate CSV of koi_light_curves_PCA_test 250\n",
      "Process of dimensionality reduction completed\n",
      "CPU times: user 7min 42s, sys: 24.3 s, total: 8min 6s\n",
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dims= [5,10,25,50,100]\n",
    "\n",
    "for d in dims:\n",
    "    model = PCA(d)\n",
    "    \n",
    "    ##################### TRAIN #####################\n",
    "    model.fit(Xtrain_std) \n",
    "    print('PCA training with', d, 'features')\n",
    "\n",
    "    #################### ARRIBA ENTRENAMIENTO ###########\n",
    "    df = pd.DataFrame(model.transform(Xtrain_std),columns=[\"Component \"+str(t+1) for t in np.arange(d)])\n",
    "    df.to_csv(\"../Datos/PCA_unb/koi_light_curves_FPCA_train\"+str(d)+\"_model.csv\",index=False)\n",
    "    print('Generate CSV of koi_light_curves_PCA_train', d)\n",
    "    \n",
    "    ##################### TEST ########################\n",
    "    df = pd.DataFrame(model.transform(Xtest_std),columns=[\"Component \"+str(t+1) for t in np.arange(d)])\n",
    "    df.to_csv(\"../Datos/PCA_unb/koi_light_curves_FPCA_test\"+str(d)+\"_model.csv\",index=False)\n",
    "    print('Generate CSV of koi_light_curves_PCA_test', d)\n",
    "    \n",
    "print('Process of dimensionality reduction completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA\n",
    "---\n",
    "#### Generate 3 files in 3 dimension: 5, 10 and 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
       "    n_components=5, random_state=None, tol=0.0001, w_init=None,\n",
       "    whiten=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastICA(5,whiten=True)\n",
    "#model.fit(Xtrain_std)  #with std -- no funciona\n",
    "#model.fit(Xtrain_std2) #without std -- funciona \n",
    "#model.fit(Xtrain_fourier) #fft -- funciona\n",
    "model.fit(X_train)  #light curve -- funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICA trainig with 20 features\n",
      "Generate CSV Train 20\n",
      "Generate CSV Test 20\n",
      "ICA trainig with 25 features\n",
      "Generate CSV Train 25\n",
      "Generate CSV Test 25\n",
      "ICA trainig with 50 features\n",
      "Generate CSV Train 50\n",
      "Generate CSV Test 50\n",
      "Process of dimensionality reduction completed\n",
      "CPU times: user 1h 24min 39s, sys: 10min 3s, total: 1h 34min 42s\n",
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "dims= [5,10,15,20,25,50]\n",
    "\n",
    "for d in dims:\n",
    "    model = FastICA(d,whiten=True)\n",
    "    \n",
    "    ##################### TRAIN #####################\n",
    "    model.fit(Xtrain_std)\n",
    "    print('ICA trainig with', d, 'features')\n",
    "\n",
    "    #################### ARRIBA ENTRENAMIENTO ###########\n",
    "    df = pd.DataFrame(model.transform(Xtrain_std),columns=[\"Component \"+str(t+1) for t in np.arange(d)])\n",
    "    df.to_csv(\"../Datos/ICA_unb/koi_light_curves_FICA_train\"+str(d)+\"_model.csv\",index=False)\n",
    "    print('Generate CSV Train', d)\n",
    "\n",
    "    ##################### TEST ########################\n",
    "    df = pd.DataFrame(model.transform(Xtest_std),columns=[\"Component \"+str(t+1) for t in np.arange(d)])\n",
    "    df.to_csv(\"../Datos/ICA_unb/koi_light_curves_FICA_test\"+str(d)+\"_model.csv\",index=False)\n",
    "    print('Generate CSV Test', d)\n",
    "    \n",
    "print('Process of dimensionality reduction completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OwnFATS (Manual)\n",
    "---\n",
    "#### Generate Own FATS features of light curve (manualy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import FATS\n",
    "\n",
    "time_ex = np.arange(3000)\n",
    "magnitude_ex = np.random.rand(3000)\n",
    "\n",
    "lc_example = np.array([magnitude_ex, time_ex])\n",
    "\n",
    "time_start = time.time()\n",
    "#a = FATS.FeatureSpace(Data=['magnitude','time'],excludeList= ['SlottedA_length','PeriodLS','StetsonK_AC',\"Period_fit\",\"Psi_CS\",\"Psi_eta\"])\n",
    "#SACAR DE EXCLUDE PERIODLS\n",
    "a = a.calculateFeature(lc_example)\n",
    "print(\"Termino en %f segundos\"%(time.time()-time_start))\n",
    "a.result(method='dict')\n",
    "\n",
    "#it take too long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.signal import resample\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def amplitude(magnitudes):\n",
    "    return 0.5 * (np.max(magnitudes) - np.min(magnitudes))\n",
    "\n",
    "def median_absolute_deviation(magnitudes):\n",
    "    median = np.median(magnitudes)\n",
    "    deviations = magnitudes - median\n",
    "    absolute_deviations = np.absolute(deviations)\n",
    "    return np.median(absolute_deviations)\n",
    "\n",
    "def residual_bright_faint_ratio(magnitudes):    # median as a fit\n",
    "    mean = np.mean(magnitudes)\n",
    "    brighter = magnitudes[magnitudes > mean]\n",
    "    fainter = magnitudes[magnitudes < mean]\n",
    "\n",
    "    resid_brighter = np.mean(np.square(brighter - mean))\n",
    "    resid_fainter = np.mean(np.square(fainter - mean))\n",
    "\n",
    "    ratio = resid_fainter / (resid_brighter+1e-14)\n",
    "    return ratio\n",
    "\n",
    "def own_fats(sequence):\n",
    "    time_ex =  np.arange(len(sequence))\n",
    "\n",
    "    minim=np.min(sequence)\n",
    "    maxim = np.max(sequence)\n",
    "    mean = np.mean(sequence)\n",
    "    std = np.std(sequence)\n",
    "    iqr = stats.iqr(sequence) #q31\n",
    "    skew = stats.skew(sequence)\n",
    "    kurt = stats.kurtosis(sequence)\n",
    "    q1 = np.percentile(sequence, 25)\n",
    "    q2 = np.percentile(sequence, 50)\n",
    "    model = LinearRegression(normalize=True,n_jobs=-1)\n",
    "    model.fit(time_ex.reshape(-1,1),sequence)\n",
    "    slope = model.coef_[0]\n",
    "    #new features\n",
    "    ampl = amplitude(sequence)\n",
    "    mad = median_absolute_deviation(sequence)\n",
    "    br_fa = residual_bright_faint_ratio(sequence)\n",
    "    median = np.median(sequence)\n",
    "    return np.array([minim,maxim,mean,std,iqr,skew,kurt,q1,q2,slope,ampl,mad,br_fa,median])\n",
    "\n",
    "def metadata_columns(match,array):\n",
    "    df = pd.read_csv(\"../Datos/\"+file_name_metadata)\n",
    "    metadata = df[(df[\"KOI Name\"] == match)].values[0][3:]\n",
    "    return np.hstack((array,metadata))\n",
    "\n",
    "columns_FATS = [\"Minimum\",\"Maximum\",\"Mean\",\"Std\",\"IQR\",\"Skew\",\"Kurtosis\",\"Q1\",\"Q2\",\"Slope\",\"Amplitude\",\"MAD\",\n",
    "               \"Residual Bright Faint Ratio\",\"Median\"]\n",
    "aux = pd.read_csv(\"../Datos/\"+file_name_metadata)\n",
    "columns_metadata = list(aux.columns[3:])\n",
    "\n",
    "def extract_FATS(X,df_label):\n",
    "    X_fats = []\n",
    "    for sequence,match in zip(X,df_label[\"KOI Name\"]):\n",
    "        aux = own_fats(sequence)\n",
    "        final = metadata_columns(match,aux)\n",
    "        X_fats.append(final)\n",
    "    return np.asarray(X_fats)\n",
    "\n",
    "def save(name_set,features,df_label):\n",
    "    df2save = pd.DataFrame(features,columns=columns_FATS+columns_metadata)\n",
    "    df2save[\"KOI Name\"] = df_label[\"KOI Name\"]\n",
    "    df2save.to_csv(\"../Datos/OwnFats_unb/koi_light_curves_FATS2_metadata_\"+name_set+\"_p_error.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 21min 21s, sys: 6min 26s, total: 2h 27min 47s\n",
      "Wall time: 6min 9s\n",
      "Training already extracted\n",
      "CPU times: user 46min 49s, sys: 2min 7s, total: 48min 57s\n",
      "Wall time: 2min 2s\n",
      "Validation already extracted\n"
     ]
    }
   ],
   "source": [
    "%time features = extract_FATS(lc_train,df_label_train) #train\n",
    "save(\"train_model\",features,df_label_train)\n",
    "print(\"Training already extracted\")\n",
    "\n",
    "%time features = extract_FATS(lc_test,df_label_test) #test\n",
    "save(\"test_model\",features,df_label_test)\n",
    "print(\"Validation already extracted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
